# -*- coding: utf-8 -*-
"""Employee Attrition Predictive Modeling_Baburina

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_INkop2P4lWB47sW0Qx-30w_7Pl7qwzF

#**Loading the Data**

The dataset from Kaggle.com was used for creating this predictive model
https://www.kaggle.com/tejashvi14/employee-future-prediction.
A company's HR department wants to predict whether some employees would leave the company in next 2 years. The task is to build a predictive model that predicts the prospects of future and present employee.
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#Loading the data
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import math
df=pd.read_csv('/content/drive/MyDrive/Files/Employee.csv')

df.head()

"""**Data:**
**"Education" **
**"JoiningYear"**
**"City"** 
**"PaymentTier"**
**"Age"** 
**"Gender"**
**"EverBenched"**
**"ExperienceInCurrentDomain"**
**"LeaveOrNot"** - Target field - predicting if the employee will leave the company

#**EDA and Data Prep**
"""

#Identifying the datatype of variables
df.dtypes

df.shape

#Checking to see if we have any duplicate rows in our DS
duplicate_rows_df=df[df.duplicated()]
print('Number of duplicated rows: , duplicate_rows_df.shape')
df.count()

#Checking to see if we have any missing values in our DS
print(df.isnull().sum())

df.nunique()

df.describe()

#Renaming some columns
df=df.rename(columns={'ExperienceInCurrentDomain':'Experience', 'LeaveOrNot':'LeftCompany'})
df

# How balanced the target column is
df['LeftCompany'].value_counts()

labels = df['LeftCompany'].value_counts().index
size = df['LeftCompany'].value_counts().values
plt.pie(size,colors= ['green', 'red'], labels = labels,autopct = "%.2f%%")
plt.title('LeftCompany')
plt.legend()

!pip install -U dataprep

from dataprep.eda import *

plot(df)

#Checking correlation
plot_correlation(df)

plt.figure(figsize = (20,10))
sns.heatmap(df.corr(),annot = True)

plot(df, 'Education', 'LeftCompany')

"""We can see that the employees with Masters degree are more inclined to leave the company comparing to employees with other levels of education. Employees with Master's degree might feel more confident in their ability to find a different job, as they have high level of education."""

plot(df, 'JoiningYear', 'LeftCompany')

"""Almost all employees who had started working in the company in 2018, left the company."""

plot(df, 'PaymentTier', 'LeftCompany')

"""Employees with the lowest and the highest income level are the most loyal to the company, they are not changing jobs so easy, while employees with the middle payment tier will leave the company with 60% probability."""

plot(df, 'Gender', 'LeftCompany')

"""Almost 50% of female employees left the company."""

plot(df, 'EverBenched', 'LeftCompany')

"""Employees who had been benched, left the company with 50% probability. It is possible that they might have been benched due to luck of qualification or due to some mistakes."""

plot(df, 'Experience', 'LeftCompany')

"""The highest percentage of employees are leaving the company after having worked at the company for two - three years."""

#Pandas Profiling
!pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip
#restart and run again

from pandas_profiling import ProfileReport
profile=ProfileReport(df, title="Pandas Profiling Report", minimal = True)
profile.to_notebook_iframe()

!pip install autoviz

from autoviz.AutoViz_Class import AutoViz_Class
AV = AutoViz_Class()

#Visualizing the correlation between different fields
dft = AV.AutoViz('/content/drive/MyDrive/Files/Employee.csv')

"""**Conclusion:**
We need to find a solution for binary classification task on unbalanced data("Left Company" 66% No, 34% Yes). There are no missing enties in this dataset, so we will keep all the data and all the columns/features for the model buidling.

#**Building the Model**

#**CatBoost**
"""

!pip install catboost

#Creating a training set for modeling and validation set to check model performance
X = df.drop(['LeftCompany'], axis=1)
y = df['LeftCompany']
from sklearn.model_selection import train_test_split
train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.8, random_state=123)

##Look at the data type of variables
train_X.dtypes

cat_feature_type= np.where(X.dtypes != np.float)[0]

#Importing library and building the model
from catboost import CatBoostClassifier
model = CatBoostClassifier(iterations=50)

model.fit(train_X, train_y,cat_features=cat_feature_type,eval_set=(test_X, test_y),plot=True)

preds_class = model.predict(test_X)

preds_proba = model.predict_proba(test_X)

from sklearn.metrics import roc_auc_score
probabilities = model.predict_proba(test_X)
roc_auc_score(test_y, probabilities[:, 1])

"""**Conclusion:**
**CatBoost** framework provides good level of prediction with **AUC= 0.87.** There are many categorical features in the datasent, so it is advanageous to use **CatBoost** framework, as no transformation is needed, it is done automatically by this framework. We will keep this model for the final comparision.

#**XGBoost**
"""

#Changing the type to numeric for categorical columns
from sklearn.preprocessing import LabelEncoder
cols = ("Education","City","Gender","EverBenched")
for c in cols:
    label = LabelEncoder()
    label.fit(list(df[c].values))
    df[c] = label.transform(list(df[c].values))

df.info()

#Creating a training set for modeling and validation set to check model performance
X = df.drop(['LeftCompany'], axis=1)
y = df['LeftCompany']
from sklearn.model_selection import train_test_split
train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.8, random_state=123)

import xgboost as xgb
from xgboost import plot_tree
from xgboost import XGBClassifier
model = XGBClassifier()

model.fit(X, y)

plot_tree(model)

preds_1 = model.predict(X)

#Printing the metrics on train data
from sklearn.metrics import roc_auc_score
probabilities = model.predict_proba(test_X)
roc_auc_score(test_y, probabilities[:, 1])

"""**Conclusion:**
Model created with the help of **XGBoost** provides reasonably good prediction level with **AUC=0.87**, so we will keep this model for the final comparision.

#**Random Forest**
"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(train_X, train_y)

predicted = rf.predict(test_X)
rf.score(test_X, test_y)

probabilities = rf.predict_proba(test_X)
roc_auc_score(test_y, probabilities[:, 1])

from sklearn import metrics  
from sklearn.metrics import roc_curve
fpr, tpr, _ = roc_curve(test_y, probabilities[:, 1])
plt.plot(fpr, tpr)
plt.plot([0,1], [0,1], color='green', lw=1, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
metrics.auc(fpr, tpr)

disp = metrics.plot_confusion_matrix(rf,test_X,test_y,values_format = 'n', display_labels = ["Still working", "Left Company"])

"""Model created with **Random Forest** algorithm provides quite good prediction result with **AUC=0.85**, we are keeping this model for the final comparision.

# **PyCaret**
"""

!pip install PyYAML==5.4.1

!pip install pycaret # change runtime type and run again

from pycaret.classification import *

#Sampling 95% for training and 5% for validation - unseen data that model will not see during the training
data = df.sample(frac=0.95, random_state=786)
data_unseen = df.drop(data.index)
data.reset_index(inplace=True, drop=True)
data_unseen.reset_index(inplace=True, drop=True)
print('Data for Modeling: ' + str(data.shape))
print('Unseen Data For Predictions: ' + str(data_unseen.shape))

#Using original dataset df
exp_clf101 =setup (df, target= "LeftCompany", normalize = True, session_id=1,
                  silent=True)

best_model = compare_models(sort='AUC')

"""**Top 5 models generated by PyCaret, using AUC as a criterium:**
1. Light Gradient Boosting Machine
2. CatBoost Classifier
3. Gradient Boosting Classifier
4. Random Forest Classifier
5. Extremely Randomized Trees

Let's tune hyperparameters for these models and find out what features are the most important in these models.
"""

lightgbm = create_model('lightgbm', fold=5)

catboost = create_model('catboost', fold=5)

gbc = create_model('gbc', fold=5)

rf = create_model('rf', fold=5)

et = create_model('et', fold=5)

#Hyperparameters optimization
tuned_lightgbm = tune_model(lightgbm)

tuned_catboost = tune_model(catboost)

tuned_gbc = tune_model(gbc)

tuned_rf= tune_model(rf)

tuned_et = tune_model(et)

#Feature importance
plot_model(tuned_lightgbm, plot='feature')

#model accuracy
plot_model(tuned_lightgbm)

evaluate_model(tuned_lightgbm)

plot_model(tuned_catboost, plot='feature')

plot_model(tuned_catboost)

plot_model(tuned_gbc, plot='feature')

plot_model(tuned_gbc)

plot_model(tuned_rf, plot='feature')

plot_model(tuned_rf)

plot_model(tuned_et, plot='feature')

plot_model(tuned_et)

#Predicting on the test set using the best model
predict_model(tuned_lightgbm)

#Finalizing the best model
final_lightgbm = finalize_model(tuned_lightgbm)

predict_model(final_lightgbm)

#Predicting on unseen data
unseen_predictions = predict_model(final_lightgbm, data=data_unseen)
unseen_predictions.head(20)

save_model(final_lightgbm,'EmployeePrediction')

"""**Conclusion:**
The best result is provided by the **Light Boosting Model - LightBoost (AUC=0.88**), generated by PyCaret. The most important features in various modls are **Gender, City, JoiningYear, Education, PaymentTier**.

#**AutoML H2Ðž**
"""

!pip install h2o
import h2o
from h2o.automl import H2OAutoML
h2o.init()

h2o_df = h2o.H2OFrame(df)

train, test = h2o_df.split_frame(ratios=[0.8], seed=1)

y = 'LeftCompany'
x = h2o_df.columns
x.remove(y)

#Binary classification
train[y] = train[y].asfactor()
test[y] = test[y].asfactor()

aml = H2OAutoML(balance_classes=True,max_runtime_secs = 60, seed = 1)
aml.train(x=x, y=y, training_frame=train, leaderboard_frame=test)

aml.explain(test)

h2o.save_model(aml.leader, path ='/content/drive/MyDrive/Files' )

test_pred = aml.predict(test)

predict = test_pred.as_data_frame()
predict

test.head()

#Adding the predicted column to the dataset
test_predict = test.cbind(test_pred)

test_predict.head()

test_predict_csv_string = test_predict.get_frame_data()
test_predict_pd = test_predict.as_data_frame(use_pandas=True)

test_predict_csv_string

test_predict_pd

"""**Conclusion:** 
The best result is provided by **Gradient Boosting Model - GBM, with AUC=0.87**, this model is generated by **AutoML from H2O**. The most important features are the following: JoiningYear, PaymentTier, City, Education, and Age**.

**Conclusion and Final Comments**

The goal of this task was to create a model to predict if an employee will leave the company within 2 years. The following frameworks were used for the model creation: **CatBoost, Random Forest, XGBoost, PyCaret, H2O.** The best model is **PyCaret LightBoost Model** with **AUC=0.88**. 
The second place is split by three models, all with the same predictive criterium (**AUC=0.87**): **CatBoost, Gradient Boosting Machine, and XGBoost model.**
**Random Forest Model* provides **AUC=0.85.**
The most important features are: **Gender, City, JoiningYear, Education, PaymentTier.**
"""